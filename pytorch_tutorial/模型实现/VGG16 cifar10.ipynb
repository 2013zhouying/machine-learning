{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG 16模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 该模型在论文《VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION》中实现\n",
    "[VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION](https://arxiv.org/abs/1409.1556)\n",
    "\n",
    "提出了一种非常大的神经网络模型，并且卷积核都为`3*3`，同时pool使用`2*2`进行最大池化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets,transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10('data/cifar10', download=True,train=True, \n",
    "                                 transform=transforms.Compose([\n",
    "                                     transforms.RandomSizedCrop(32),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                 ]))\n",
    "test_dataset = datasets.CIFAR10('data/cifar10', \n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,shuffle=True,batch_size=64)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建vgg16模型\n",
    "Vgg16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "# 这是前面的卷积和池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16,self).__init__()\n",
    "        self.features = self._make_layers()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3072, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 10),\n",
    "        )   \n",
    "    \n",
    "    def forward(self,x):\n",
    "        self.features(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _make_layers(self):\n",
    "        layers = []\n",
    "        in_channel = 3\n",
    "        for x in Vgg16:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2,stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channel, x, kernel_size=3, padding=1),\n",
    "                          nn.ReLU(inplace=True)]\n",
    "                in_channel = x\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG16 (\n",
       "  (features): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU (inplace)\n",
       "    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU (inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU (inplace)\n",
       "    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU (inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU (inplace)\n",
       "    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU (inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU (inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU (inplace)\n",
       "    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU (inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU (inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU (inplace)\n",
       "    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Linear (3072 -> 4096)\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Dropout (p = 0.5)\n",
       "    (3): Linear (4096 -> 4096)\n",
       "    (4): ReLU (inplace)\n",
       "    (5): Dropout (p = 0.5)\n",
       "    (6): Linear (4096 -> 10)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2.308626413345337\n",
      "0 1 2.322632312774658\n",
      "0 2 2.30881404876709\n",
      "0 3 2.3024017810821533\n",
      "0 4 2.304530620574951\n",
      "0 5 2.312730312347412\n",
      "0 6 2.3132405281066895\n",
      "0 7 2.3081960678100586\n",
      "0 8 2.2934482097625732\n",
      "0 9 2.307502508163452\n",
      "0 10 2.2964401245117188\n",
      "0 11 2.3000569343566895\n",
      "0 12 2.293687105178833\n",
      "0 13 2.3019235134124756\n",
      "0 14 2.299318552017212\n",
      "0 15 2.3069067001342773\n",
      "0 16 2.2907907962799072\n",
      "0 17 2.3125104904174805\n",
      "0 18 2.2795302867889404\n",
      "0 19 2.2819275856018066\n",
      "0 20 2.3029062747955322\n",
      "0 21 2.28049373626709\n",
      "0 22 2.296945810317993\n",
      "0 23 2.255983829498291\n",
      "0 24 2.2826952934265137\n",
      "0 25 2.2702181339263916\n",
      "0 26 2.276179313659668\n",
      "0 27 2.2933664321899414\n",
      "0 28 2.289134979248047\n",
      "0 29 2.2786049842834473\n",
      "0 30 2.281862735748291\n",
      "0 31 2.2685046195983887\n",
      "0 32 2.2810394763946533\n",
      "0 33 2.28056263923645\n",
      "0 34 2.2981977462768555\n",
      "0 35 2.271517038345337\n",
      "0 36 2.268321990966797\n",
      "0 37 2.297367572784424\n",
      "0 38 2.2930243015289307\n",
      "0 39 2.263733148574829\n",
      "0 40 2.277125835418701\n",
      "0 41 2.2685296535491943\n",
      "0 42 2.2477917671203613\n",
      "0 43 2.2765448093414307\n",
      "0 44 2.2628414630889893\n",
      "0 45 2.2926034927368164\n",
      "0 46 2.273364782333374\n",
      "0 47 2.2911722660064697\n",
      "0 48 2.266204595565796\n",
      "0 49 2.2828969955444336\n",
      "0 50 2.2835562229156494\n",
      "0 51 2.2348763942718506\n",
      "0 52 2.2739179134368896\n",
      "0 53 2.2760913372039795\n",
      "0 54 2.265413522720337\n",
      "0 55 2.240095376968384\n",
      "0 56 2.261798858642578\n",
      "0 57 2.2315409183502197\n",
      "0 58 2.237117290496826\n",
      "0 59 2.217177391052246\n",
      "0 60 2.2368147373199463\n",
      "0 61 2.2705397605895996\n",
      "0 62 2.249795436859131\n",
      "0 63 2.2446281909942627\n",
      "0 64 2.268911123275757\n",
      "0 65 2.2187070846557617\n",
      "0 66 2.2410531044006348\n",
      "0 67 2.2216134071350098\n",
      "0 68 2.233691453933716\n",
      "0 69 2.2382307052612305\n",
      "0 70 2.252113103866577\n",
      "0 71 2.2322187423706055\n",
      "0 72 2.2632739543914795\n",
      "0 73 2.2747092247009277\n",
      "0 74 2.2158761024475098\n",
      "0 75 2.2131035327911377\n",
      "0 76 2.2359182834625244\n",
      "0 77 2.236250162124634\n",
      "0 78 2.2366888523101807\n",
      "0 79 2.227047920227051\n",
      "0 80 2.251549005508423\n",
      "0 81 2.2686409950256348\n",
      "0 82 2.238807439804077\n",
      "0 83 2.2233545780181885\n",
      "0 84 2.2307283878326416\n",
      "0 85 2.2062132358551025\n",
      "0 86 2.2352943420410156\n",
      "0 87 2.200451135635376\n",
      "0 88 2.2166707515716553\n",
      "0 89 2.2421302795410156\n",
      "0 90 2.229429006576538\n",
      "0 91 2.1937811374664307\n",
      "0 92 2.201784133911133\n",
      "0 93 2.264467716217041\n",
      "0 94 2.2306389808654785\n",
      "0 95 2.17117977142334\n",
      "0 96 2.2218520641326904\n",
      "0 97 2.2650134563446045\n",
      "0 98 2.195145606994629\n",
      "0 99 2.15392804145813\n",
      "0 100 2.2079615592956543\n",
      "0 101 2.2122626304626465\n",
      "0 102 2.2172348499298096\n",
      "0 103 2.1990973949432373\n",
      "0 104 2.2490334510803223\n",
      "0 105 2.1877970695495605\n",
      "0 106 2.1840856075286865\n",
      "0 107 2.1803462505340576\n",
      "0 108 2.165827751159668\n",
      "0 109 2.196267604827881\n",
      "0 110 2.183584213256836\n",
      "0 111 2.20396089553833\n",
      "0 112 2.1921191215515137\n",
      "0 113 2.18766188621521\n",
      "0 114 2.184650421142578\n",
      "0 115 2.2354331016540527\n",
      "0 116 2.20223069190979\n",
      "0 117 2.2178256511688232\n",
      "0 118 2.1695902347564697\n",
      "0 119 2.185239791870117\n",
      "0 120 2.15360951423645\n",
      "0 121 2.2057483196258545\n",
      "0 122 2.242659330368042\n",
      "0 123 2.2308850288391113\n",
      "0 124 2.1888694763183594\n",
      "0 125 2.2224009037017822\n",
      "0 126 2.253126859664917\n",
      "0 127 2.21036696434021\n",
      "0 128 2.1879003047943115\n",
      "0 129 2.1567766666412354\n",
      "0 130 2.1649036407470703\n",
      "0 131 2.1907763481140137\n",
      "0 132 2.211791753768921\n",
      "0 133 2.1908910274505615\n",
      "0 134 2.2310900688171387\n",
      "0 135 2.2401819229125977\n",
      "0 136 2.1336448192596436\n",
      "0 137 2.1607611179351807\n",
      "0 138 2.191018581390381\n",
      "0 139 2.208089590072632\n",
      "0 140 2.1974523067474365\n",
      "0 141 2.1590723991394043\n",
      "0 142 2.178825855255127\n",
      "0 143 2.2525434494018555\n",
      "0 144 2.194878101348877\n",
      "0 145 2.197864294052124\n",
      "0 146 2.2566206455230713\n",
      "0 147 2.177608013153076\n",
      "0 148 2.1894702911376953\n",
      "0 149 2.2216906547546387\n",
      "0 150 2.1481752395629883\n",
      "0 151 2.163776159286499\n",
      "0 152 2.168473482131958\n",
      "0 153 2.221423387527466\n",
      "0 154 2.1805970668792725\n",
      "0 155 2.1685235500335693\n",
      "0 156 2.1851987838745117\n",
      "0 157 2.173591375350952\n",
      "0 158 2.157413959503174\n",
      "0 159 2.1853766441345215\n",
      "0 160 2.2003204822540283\n",
      "0 161 2.152067184448242\n",
      "0 162 2.160400390625\n",
      "0 163 2.264139175415039\n",
      "0 164 2.154822826385498\n",
      "0 165 2.208176612854004\n",
      "0 166 2.133519411087036\n",
      "0 167 2.1854093074798584\n",
      "0 168 2.079444408416748\n",
      "0 169 2.177173614501953\n",
      "0 170 2.189622163772583\n",
      "0 171 2.1944494247436523\n",
      "0 172 2.144374370574951\n",
      "0 173 2.208540916442871\n",
      "0 174 2.1648731231689453\n",
      "0 175 2.1697912216186523\n",
      "0 176 2.0962958335876465\n",
      "0 177 2.1689600944519043\n",
      "0 178 2.191032886505127\n",
      "0 179 2.158397674560547\n",
      "0 180 2.1546125411987305\n",
      "0 181 2.1779284477233887\n",
      "0 182 2.168503522872925\n",
      "0 183 2.1891579627990723\n",
      "0 184 2.154092311859131\n",
      "0 185 2.2349588871002197\n",
      "0 186 2.2097551822662354\n",
      "0 187 2.156944990158081\n",
      "0 188 2.0931549072265625\n",
      "0 189 2.085411787033081\n",
      "0 190 2.162714958190918\n",
      "0 191 2.190934658050537\n",
      "0 192 2.145150899887085\n",
      "0 193 2.136756181716919\n",
      "0 194 2.1518101692199707\n",
      "0 195 2.237417459487915\n",
      "0 196 2.1590428352355957\n",
      "0 197 2.2009871006011963\n",
      "0 198 2.1371195316314697\n",
      "0 199 2.238757371902466\n",
      "0 200 2.1378633975982666\n",
      "0 201 2.148592233657837\n",
      "0 202 2.1822104454040527\n",
      "0 203 2.1226301193237305\n",
      "0 204 2.178906202316284\n",
      "0 205 2.154991865158081\n",
      "0 206 2.115644693374634\n",
      "0 207 2.053542137145996\n",
      "0 208 2.12093448638916\n",
      "0 209 2.183749198913574\n",
      "0 210 2.1787703037261963\n",
      "0 211 2.1364710330963135\n",
      "0 212 2.0799922943115234\n",
      "0 213 2.1623897552490234\n",
      "0 214 2.167384147644043\n",
      "0 215 2.1599602699279785\n",
      "0 216 2.1781744956970215\n",
      "0 217 2.1409525871276855\n",
      "0 218 2.11832857131958\n",
      "0 219 2.1402244567871094\n",
      "0 220 2.1282689571380615\n",
      "0 221 2.105891227722168\n",
      "0 222 2.1278274059295654\n",
      "0 223 2.149768590927124\n",
      "0 224 2.184457540512085\n",
      "0 225 2.1392483711242676\n",
      "0 226 2.1619458198547363\n",
      "0 227 2.0864531993865967\n",
      "0 228 2.067868709564209\n",
      "0 229 2.0516881942749023\n",
      "0 230 2.1142120361328125\n",
      "0 231 2.184103012084961\n",
      "0 232 2.0528624057769775\n",
      "0 233 2.1818172931671143\n",
      "0 234 2.1032934188842773\n",
      "0 235 2.1182291507720947\n",
      "0 236 2.153554677963257\n",
      "0 237 2.2416763305664062\n",
      "0 238 2.1946492195129395\n",
      "0 239 2.172276496887207\n",
      "0 240 2.1820292472839355\n",
      "0 241 2.076432228088379\n",
      "0 242 2.1023659706115723\n",
      "0 243 2.116848945617676\n",
      "0 244 2.1353542804718018\n",
      "0 245 2.1122493743896484\n",
      "0 246 2.235583782196045\n",
      "0 247 2.080183982849121\n",
      "0 248 2.1370232105255127\n",
      "0 249 2.0947954654693604\n",
      "0 250 2.113879919052124\n",
      "0 251 2.079561710357666\n",
      "0 252 2.2050223350524902\n",
      "0 253 2.1294357776641846\n",
      "0 254 2.0819311141967773\n",
      "0 255 2.10489821434021\n",
      "0 256 2.0318896770477295\n",
      "0 257 2.2025249004364014\n",
      "0 258 2.079373598098755\n",
      "0 259 2.129617691040039\n",
      "0 260 2.145108461380005\n",
      "0 261 2.130216121673584\n",
      "0 262 2.141066074371338\n",
      "0 263 2.0463714599609375\n",
      "0 264 2.1664905548095703\n",
      "0 265 2.082911729812622\n",
      "0 266 2.0211217403411865\n",
      "0 267 2.0682201385498047\n",
      "0 268 2.0392110347747803\n",
      "0 269 2.182065963745117\n",
      "0 270 2.083827018737793\n",
      "0 271 2.166757106781006\n",
      "0 272 2.118981122970581\n",
      "0 273 2.2129063606262207\n",
      "0 274 2.189751386642456\n",
      "0 275 2.0637366771698\n",
      "0 276 2.0641064643859863\n",
      "0 277 2.083409547805786\n",
      "0 278 2.0644006729125977\n",
      "0 279 2.0897715091705322\n",
      "0 280 2.1000635623931885\n",
      "0 281 2.1021387577056885\n",
      "0 282 2.1367857456207275\n",
      "0 283 2.192140579223633\n",
      "0 284 2.1415774822235107\n",
      "0 285 2.106245994567871\n",
      "0 286 2.165769338607788\n",
      "0 287 2.12044620513916\n",
      "0 288 2.0602173805236816\n",
      "0 289 2.078956127166748\n",
      "0 290 2.0976510047912598\n",
      "0 291 2.0704612731933594\n",
      "0 292 2.108764886856079\n",
      "0 293 2.1276164054870605\n",
      "0 294 2.150489091873169\n",
      "0 295 2.188161849975586\n",
      "0 296 2.1355175971984863\n",
      "0 297 2.0945329666137695\n",
      "0 298 2.145250082015991\n",
      "0 299 2.1333534717559814\n",
      "0 300 2.1482672691345215\n",
      "0 301 2.1606855392456055\n",
      "0 302 2.051715612411499\n",
      "0 303 2.0996994972229004\n",
      "0 304 2.1436171531677246\n",
      "0 305 2.0200319290161133\n",
      "0 306 2.0699000358581543\n",
      "0 307 2.1558332443237305\n",
      "0 308 2.131220579147339\n",
      "0 309 2.2277820110321045\n",
      "0 310 2.1427013874053955\n",
      "0 311 2.123039484024048\n",
      "0 312 2.1135566234588623\n",
      "0 313 2.0828726291656494\n",
      "0 314 2.107827663421631\n",
      "0 315 2.1623523235321045\n",
      "0 316 2.079771041870117\n",
      "0 317 2.2234582901000977\n",
      "0 318 2.0964784622192383\n",
      "0 319 2.10005521774292\n",
      "0 320 2.0442960262298584\n",
      "0 321 1.9903277158737183\n",
      "0 322 2.0437397956848145\n",
      "0 323 2.0990545749664307\n",
      "0 324 2.1908607482910156\n",
      "0 325 2.1419568061828613\n",
      "0 326 2.145944118499756\n",
      "0 327 2.0347838401794434\n",
      "0 328 2.247270345687866\n",
      "0 329 2.131804943084717\n",
      "0 330 2.1264290809631348\n",
      "0 331 2.0547547340393066\n",
      "0 332 2.0638139247894287\n",
      "0 333 2.140887498855591\n",
      "0 334 2.069793939590454\n",
      "0 335 2.076476812362671\n",
      "0 336 2.0047640800476074\n",
      "0 337 2.170184850692749\n",
      "0 338 2.106600284576416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 339 2.071108102798462\n",
      "0 340 2.034736156463623\n",
      "0 341 2.050726890563965\n",
      "0 342 2.148341655731201\n",
      "0 343 2.2341394424438477\n",
      "0 344 2.2088513374328613\n",
      "0 345 2.0736465454101562\n",
      "0 346 2.1365978717803955\n",
      "0 347 2.040461301803589\n",
      "0 348 2.099363327026367\n",
      "0 349 2.0992183685302734\n",
      "0 350 2.1853909492492676\n",
      "0 351 2.092517375946045\n",
      "0 352 2.1149511337280273\n",
      "0 353 2.0966577529907227\n",
      "0 354 2.1241352558135986\n",
      "0 355 2.0867302417755127\n",
      "0 356 2.1338083744049072\n",
      "0 357 2.043111562728882\n",
      "0 358 2.179255247116089\n",
      "0 359 2.145587205886841\n",
      "0 360 2.148932695388794\n",
      "0 361 2.0983290672302246\n",
      "0 362 2.12691593170166\n",
      "0 363 1.9912168979644775\n",
      "0 364 2.0895566940307617\n",
      "0 365 1.9415191411972046\n",
      "0 366 2.1331725120544434\n",
      "0 367 2.0540900230407715\n",
      "0 368 2.066093921661377\n",
      "0 369 2.0489680767059326\n",
      "0 370 2.1017656326293945\n",
      "0 371 2.0825273990631104\n",
      "0 372 2.119252920150757\n",
      "0 373 2.1830151081085205\n",
      "0 374 2.1857683658599854\n",
      "0 375 2.0684292316436768\n",
      "0 376 2.0069668292999268\n",
      "0 377 2.0501294136047363\n",
      "0 378 2.07235050201416\n",
      "0 379 2.029852867126465\n",
      "0 380 2.102199077606201\n",
      "0 381 2.1731679439544678\n",
      "0 382 2.052645683288574\n",
      "0 383 2.0961129665374756\n",
      "0 384 1.9698904752731323\n",
      "0 385 2.0974931716918945\n",
      "0 386 2.0213706493377686\n",
      "0 387 2.0944607257843018\n",
      "0 388 2.1429123878479004\n",
      "0 389 1.9934123754501343\n",
      "0 390 2.069399118423462\n",
      "0 391 2.095628499984741\n",
      "0 392 2.1459715366363525\n",
      "0 393 2.050550699234009\n",
      "0 394 2.1315290927886963\n",
      "0 395 2.1223957538604736\n",
      "0 396 1.9902552366256714\n",
      "0 397 2.091259241104126\n",
      "0 398 2.0203282833099365\n",
      "0 399 2.0833940505981445\n",
      "0 400 2.0882511138916016\n",
      "0 401 2.0674357414245605\n",
      "0 402 1.9900176525115967\n",
      "0 403 2.053210735321045\n",
      "0 404 2.1014456748962402\n",
      "0 405 2.094313383102417\n",
      "0 406 1.961618423461914\n",
      "0 407 1.9776103496551514\n",
      "0 408 2.134869337081909\n",
      "0 409 2.0514867305755615\n",
      "0 410 2.154588460922241\n",
      "0 411 2.086775064468384\n",
      "0 412 1.9289441108703613\n",
      "0 413 1.9865102767944336\n",
      "0 414 1.9111565351486206\n",
      "0 415 2.135119915008545\n",
      "0 416 2.070632219314575\n",
      "0 417 2.010338068008423\n",
      "0 418 2.098963737487793\n",
      "0 419 2.0060737133026123\n",
      "0 420 2.1591250896453857\n",
      "0 421 2.064401626586914\n",
      "0 422 2.0730655193328857\n",
      "0 423 2.0906758308410645\n",
      "0 424 2.0610969066619873\n",
      "0 425 2.0711734294891357\n",
      "0 426 2.0850765705108643\n",
      "0 427 2.083386182785034\n",
      "0 428 2.1235501766204834\n",
      "0 429 2.0920135974884033\n",
      "0 430 2.0340726375579834\n",
      "0 431 2.0440080165863037\n",
      "0 432 2.1357262134552\n",
      "0 433 2.1340479850769043\n",
      "0 434 2.1026477813720703\n",
      "0 435 2.0900065898895264\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-c21e1715ebda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haxu/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/haxu/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for t, (data, target) in enumerate(train_loader):        \n",
    "        data,target = Variable(data),Variable(target)\n",
    "        pred = model(data)\n",
    "        loss = loss_fn(pred,target)\n",
    "        print(epoch,t,loss.data[0])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
