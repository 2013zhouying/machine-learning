{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Scale(40),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 Dataset\n",
    "train_dataset = CIFAR10(root='data/cifar10',\n",
    "                              train=True,\n",
    "                              transform=transform,\n",
    "                              download=False)\n",
    "\n",
    "test_dataset = CIFAR10(root='data/cifar10',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义残差网络\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,stride=1):\n",
    "        super(ResidualBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size=(3,3),stride=stride,padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels,out_channels,kernel_size=(3,3),stride=1,padding=1,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 下采样\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels!=out_channels or stride!=1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                                nn.Conv2d(in_channels,out_channels,stride=stride,kernel_size=(1,1),bias=False),\n",
    "                                nn.BatchNorm2d(out_channels),\n",
    "                                         )\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CIFARResNet18(nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(CIFARResNet18,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=64,stride=1,kernel_size=(3,3),padding=1,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.stage1 = self._create_stage(64, 64, stride=1)\n",
    "        self.stage2 = self._create_stage(64, 128, stride=2)\n",
    "        self.stage3 = self._create_stage(128, 256, stride=2)\n",
    "        self.stage4 = self._create_stage(256,512, stride=2)\n",
    "        \n",
    "        self.linear = nn.Linear(512,num_classes)\n",
    "        \n",
    "        \n",
    "    def _create_stage(self, in_channels,out_channels,stride):\n",
    "        return nn.Sequential(\n",
    "            ResidualBlock(in_channels,out_channels,stride=stride),\n",
    "            ResidualBlock(out_channels,out_channels,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.stage1(out)\n",
    "        out = self.stage2(out)\n",
    "        out = self.stage3(out)\n",
    "        out = self.stage4(out)\n",
    "        out = F.avg_pool2d(out,4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = CIFARResNet18().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 [0/500] Loss : 2.280\n",
      "Epoch: 0 [10/500] Loss : 2.180\n",
      "Epoch: 0 [20/500] Loss : 2.083\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-13136518d155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    losses = []\n",
    "    start = time.time()\n",
    "    for batch_index,(inputs,targets) in enumerate(train_loader):\n",
    "        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = net(inputs)\n",
    "        loss = loss_fn(pred,targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data[0])\n",
    "        if batch_index % 10==0:\n",
    "            print(\"Epoch: %d [%d/%d] Loss : %.3f\" % (epoch,batch_index,len(train_loader), np.mean(losses)))\n",
    "    end = time.time()\n",
    "    print('Epoch: %d Loss : %.3f Time : %.3f seconds' % (epoch, np.mean(losses), end-start))\n",
    "    # eval\n",
    "    \n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_index,(inputs,targets) in enumerate(test_loader):\n",
    "        inputs, targets = Variable(inputs).cuda(), Variable(targets).cuda()\n",
    "        pred = net(inputs)\n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "    print('accuarcy: %.3f'%(100.0* correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 1.096\n",
      "Epoch : 0 Loss : 0.985\n",
      "Epoch : 0 Loss : 0.991\n",
      "Epoch : 0 Loss : 1.024\n",
      "Epoch : 0 Loss : 1.028\n",
      "Epoch : 0 Loss : 1.020\n",
      "Epoch : 0 Loss : 1.021\n",
      "Epoch : 0 Loss : 1.017\n",
      "Epoch : 0 Loss : 1.020\n",
      "Epoch : 0 Loss : 1.014\n",
      "Epoch : 0 Loss : 1.018\n",
      "Epoch : 0 Loss : 1.011\n",
      "Epoch : 0 Loss : 1.005\n",
      "Epoch : 0 Loss : 1.001\n",
      "Epoch : 0 Loss : 0.998\n",
      "Epoch : 0 Loss : 0.995\n",
      "Epoch : 0 Loss : 0.987\n",
      "Epoch : 0 Loss : 0.987\n",
      "Epoch : 0 Loss : 0.989\n",
      "Epoch : 0 Loss : 0.985\n",
      "Epoch : 0 Loss : 0.982\n",
      "Epoch : 0 Loss : 0.979\n",
      "Epoch : 0 Loss : 0.978\n",
      "Epoch : 0 Loss : 0.974\n",
      "Epoch : 0 Loss : 0.971\n",
      "Epoch : 0 Loss : 0.970\n",
      "Epoch : 0 Loss : 0.967\n",
      "Epoch : 0 Loss : 0.965\n",
      "Epoch : 0 Loss : 0.964\n",
      "Epoch : 0 Loss : 0.963\n",
      "Epoch : 0 Loss : 0.962\n",
      "Epoch : 0 Loss : 0.962\n",
      "Epoch : 0 Loss : 0.960\n",
      "Epoch : 0 Loss : 0.958\n",
      "Epoch : 0 Loss : 0.955\n",
      "Epoch : 0 Loss : 0.953\n",
      "Epoch : 0 Loss : 0.949\n",
      "Epoch : 0 Loss : 0.946\n",
      "Epoch : 0 Loss : 0.945\n",
      "Epoch : 0 Loss : 0.943\n",
      "Epoch : 0 Loss : 0.939\n",
      "Epoch : 0 Loss : 0.936\n",
      "Epoch : 0 Loss : 0.935\n",
      "Epoch : 0 Loss : 0.932\n",
      "Epoch : 0 Loss : 0.930\n",
      "Epoch : 0 Loss : 0.928\n",
      "Epoch : 0 Loss : 0.926\n",
      "Epoch : 0 Loss : 0.923\n",
      "Epoch : 0 Loss : 0.920\n",
      "Epoch : 0 Loss : 0.918\n",
      "Epoch : 0 Loss : 0.916 Time : 249.014 seconds \n",
      "Epoch : 0 Test Acc : 66.110\n",
      "--------------------------------------------------------------\n",
      "Epoch : 1 Loss : 0.898\n",
      "Epoch : 1 Loss : 0.812\n",
      "Epoch : 1 Loss : 0.796\n",
      "Epoch : 1 Loss : 0.793\n",
      "Epoch : 1 Loss : 0.790\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a6cc7726983e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch : %d Loss : %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(200):\n",
    "    losses = []\n",
    "    # Train\n",
    "    start = time.time()\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data[0])\n",
    "        if batch_idx%10 == 0:\n",
    "            print('Epoch : %d Loss : %.3f' % (epoch, np.mean(losses)))\n",
    "    end = time.time()\n",
    "\n",
    "    print('Epoch : %d Loss : %.3f Time : %.3f seconds ' % (epoch, np.mean(losses), end - start))\n",
    "    # Evaluate\n",
    "    net.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets, volatile=True)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    print('Epoch : %d Test Acc : %.3f' % (epoch, 100.*correct/total))\n",
    "    print('--------------------------------------------------------------')\n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
