{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在这一系列notebook中，我们会从使用Numpy来搭建最简单的同时最直观的神经网络，包括实现各种神经网络的技巧,内容包括\n",
    "\n",
    "* relu\n",
    "* softmax\n",
    "* dropout\n",
    "* maxNorm\n",
    "* batchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 首先导入库 Numpy以及 plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义抽象神经网络层，该层什么东西也不做，接下来relu，dense层继承该类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "    def backward(self, input, grad_output):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义一个激活层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义Relu层\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,input):\n",
    "        return np.maximum(0,input)\n",
    "    def backward(self,input,grad_output):\n",
    "        relu_grad = input>0\n",
    "        return grad_output*relu_grad\n",
    "    \n",
    "# 定义Sigmoid层\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _sigmoid(self,x):\n",
    "        return 1.0/(1+np.exp(-x))\n",
    "    \n",
    "    def forward(self,input):\n",
    "        return self._sigmoid(input)\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        sigmoid_grad = self._sigmoid(input)*(1-self._sigmoid(input))\n",
    "        return grad_output*sigmoid_grad\n",
    "\n",
    "# 定义Tanh层\n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def _tanh(self,x):\n",
    "        return np.tanh(x)\n",
    "    def forward(self,input):\n",
    "        return self._tanh(input)\n",
    "    def backward(self, input, grad_output):\n",
    "        grad_tanh = 1-(self._tanh(input))**2\n",
    "        return grad_output*grad_tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 定义Dense\n",
    "\n",
    "> 根据论文\n",
    "[xavier](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 而实现的一种初始化方法，该论文认为初始化数值与输入节点个数和输出节点个数有关\n",
    "\n",
    "$$ \\sqrt{\\frac{2}{in+out}} $$\n",
    "\n",
    "> 每一个前向传播为 \n",
    "\n",
    "$$  a = X \\cdot W + b $$\n",
    "\n",
    "> 而反向传播为将前一层的残差传播过来，进行修正\n",
    "\n",
    "$$ \\frac{dL}{dw} = \\frac{dL}{d Dense} \\frac{d Dense}{dw} $$ 这里的 $ \\frac{Ddense}{dw} $ 为 $x.T$,$ \\frac{dL}{d Dense} $ 就是后面一层传过来的残差，同时在该层需要将残差往前传播。\n",
    "\n",
    "* 使用随机梯度下降算法来进行学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1,init='xavier'):\n",
    "        self.learning_rate = learning_rate\n",
    "        if init=='xavier':\n",
    "            self.weights = np.random.randn(input_units, output_units)*np.sqrt(2./(input_units+output_units))\n",
    "        else: \n",
    "            self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "    def forward(self,input):\n",
    "        return np.dot(input,self.weights)+self.biases\n",
    "    def backward(self,input,grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(input.T,grad_output)/input.shape[0]\n",
    "        grad_biases = grad_output.mean(axis=0)\n",
    "        \n",
    "        ### sgd #####\n",
    "        self.weights = self.weights - self.learning_rate*grad_weights\n",
    "        self.biases = self.biases - self.learning_rate*grad_biases\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 计算交叉熵\n",
    "$$\\frac{dCE(y,y^{pred})}{d\\theta} = \\left\\{\\begin{array}{cc} \n",
    "\t\ty^{pred} - 1, & i=k\\\\ \n",
    "\t\ty^{pred}, & other\\ values \n",
    "\t\\end{array}\\right.$$\n",
    "    \n",
    "    $$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
    "    \n",
    "    $$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return - ones_for_answers + softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 神经网络的前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward(network,X):\n",
    "    activations = []\n",
    "    input = X\n",
    "    for layer in network:\n",
    "        activations.append(layer.forward(input))\n",
    "        input = activations[-1]\n",
    "                \n",
    "    assert len(activations) == len(network)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 神经网络预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(network,X):\n",
    "    logits = forward(network,X)[-1]\n",
    "    return logits.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 训练\n",
    "def train(network,X,y):    \n",
    "    layer_activations = forward(network,X)\n",
    "    layer_inputs = [X]+layer_activations  \n",
    "    logits = layer_activations[-1]\n",
    "    \n",
    "    loss = softmax_crossentropy_with_logits(logits,y)\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits,y)\n",
    "    \n",
    "    for layer_i in range(len(network))[::-1]:\n",
    "        layer = network[layer_i]\n",
    "        loss_grad = layer.backward(layer_inputs[layer_i],loss_grad) #grad w.r.t. input, also weight updates\n",
    "        \n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现dropout\n",
    "\n",
    "* 根据论文[dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) 使用dropout能够防止过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self,p):\n",
    "        self.p = p\n",
    "    def forward(self,input,mode='test'):\n",
    "        self.mode = mode\n",
    "        if self.mode=='test':\n",
    "            return input\n",
    "        self.mask = (np.random.rand(*input.shape)>=self.p)/(1-self.p)\n",
    "        input = input * self.mask\n",
    "        return input\n",
    "    def backward(self,input,grad_output):\n",
    "        return grad_output*self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现各种优化算法，使用低耦合 高内聚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw,b,db, config=None):\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "    \n",
    "    w -= config['learning_rate'] * dw\n",
    "    b -= config['learning_rate'] * db\n",
    "    return w, b, config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 同时，Dense内权值更新就要改变一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1,init='xavier',optim=sgd):\n",
    "        self.learning_rate = learning_rate\n",
    "        if init=='xavier':\n",
    "            self.weights = np.random.randn(input_units, output_units)*np.sqrt(2./(input_units+output_units))\n",
    "        else: \n",
    "            self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "        self.optim = optim\n",
    "        self.config = None\n",
    "        \n",
    "    def forward(self,input,mode):\n",
    "        return np.dot(input,self.weights)+self.biases\n",
    "    def backward(self,input,grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(input.T,grad_output)/input.shape[0]\n",
    "        grad_biases = grad_output.mean(axis=0)   \n",
    "        self.weights,self.biases,self.config = optim(self.weights,grad_weights,self.biases,self.grad_biases,self.config)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1,init='xavier',optim=sgd,maxnorm=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        if init=='xavier':\n",
    "            self.weights = np.random.randn(input_units, output_units)*np.sqrt(2./(input_units+output_units))\n",
    "        else: \n",
    "            self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "        self.optim = optim\n",
    "        self.config = None\n",
    "        self.maxnorm = False\n",
    "        \n",
    "    def forward(self,input,mode):\n",
    "        return np.dot(input,self.weights)+self.biases\n",
    "    def backward(self,input,grad_output):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(input.T,grad_output)/input.shape[0]\n",
    "        grad_biases = grad_output.mean(axis=0)   \n",
    "        self.weights,self.biases,self.config = optim(self.weights,grad_weights,self.biases,self.grad_biases,self.config)\n",
    "        \n",
    "        if self.maxnorm:\n",
    "            norms = np.sqrt(np.sum(np.square(self.weights), 0, keepdims=True))\n",
    "            desired = np.clip(norms, 0, 2)\n",
    "            self.weights *= (desired / (1e-7 + norms)) \n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
